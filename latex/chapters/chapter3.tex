\chapter{Theory for LASSO in high dimensions}
\label{chapter: LASSO}
In chapter \ref{chapter: classical theory of linear regression}, we enjoyed the nice setting of square systems (\(n = p\)). In this chapter however, we work with underdetermined systems, which means that we have few observations but each with many components. In fact, this difference may even be substantial and we will introduce general methods to deal with such cases. Several linear regresion methods exist in order to work for underdetermined systems but we will introduce only one, namely LASSO. Our exploration will consist of two parts. In the first one, we will work with the heuristic assumption that the underlying true model is linear. In the second part, we will acknowledge that the underlying true model is not linear, but we will argue and show that it can be well-approximated by a linear model, accounting for the additional nonlinear part. \\
LASSO (Least Absolute Shrinkage and Selection Operator) is a linear regression method which takes into account the \(\ell_1\)-norm of the estimator \(\bta\) as a penalization parameter. LASSO aims at minimizing the following quantity over \(\bta\):
\[
    \frac{\|\Y-\X \bta\|_{2}^{2}}{n}+\lambda\|\bta\|_{1}
\]
Because of the \(\ell_1\) term, the model is able to indentify and select the relevant parameters, while the other parameters can be set to zero. This makes LASSO shine, in comparison to other (non-penalized) linear regression techniques, when there is a great number of parameters. \\
Some parts of this chapter follow \textit{Statistics for High-Dimensional Data} by Peter BÃ¼hlmann and Sara van de Geer.

In this section, we assume that there exists some underlying ``true value" for the parameter \(\bta\) that would make the model ideally fit the observations to the predictions. We call this parameter vector \(\bta^0\). We want to be as close as possible from such a parameter. \\
One of the roadblocks that prevents us from finding \(\bta^0\) is that we do not know which of the \(\bta_j\)'s are non-zero. Therefore, one of our goals is to identify the so-called \textit{active set}, which is the set of non-zero components of \(\bta^0\). We write it \(S_0\) and we define it as follows:
\[
    S_0 := \left\{ 1 \leq j \leq p : \bta_j^0 \neq 0 \right\}
\]
Let us also introduce the number of non-zero parameters \(s_0\). We call \(s_0\) the sparsity index of \(\bta^0\) and it is defined as the cardinality of \(S_0\), that is \(s_0 := |S_0|\). \\
We must account for the fact that we do not know \(S_0\). Using a regularization parameter fulfills that purpose as it ensures that the components of our estimator will remain ``small''. Thus, keeping the results from chapter \ref{chapter: classical theory of linear regression} in mind, we define $\btahat$ as follows:
\begin{equation}
    \label{eqn: beta hat (LASSO)}
    \btahat:=\arg \min _{\bta}\left\{ \frac{\|\Y-\X \bta\|_{2}^{2}}{n}+\lambda\|\bta\|_{1} \right\}
\end{equation}

\section{Assuming the truth is linear}
\label{section: assuming truth is linear}
We will build several results from the following lemma, which we logically call the Basic Inequality.
\begin{lemma}[Basic Inequality]
    \label{lemma: basic inequality}
    The following holds:
    \[
        \frac{\| \X (\btahat-\bta^{0})\|_{2}^{2}}{n} + \lambda\|\btahat\|_{1} \leq 2 \frac{\varepsilon^{T} \X (\btahat-\bta^{0})}{n}+\lambda \| \bta^{0} \|_{1}
    \]
\end{lemma}
\begin{proof}
    By definition of $\btahat$, we have that
    \[
        \forall \bta \quad \frac{\| \Y-\X \btahat \|_{2}^{2}}{n} + \lambda \| \btahat \|_{1} \leq \frac{\|\Y-\X \bta \|_{2}^{2}}{n}+\lambda \| \bta \|_{1}
    \]
    In particular for $\bta = \bta^0$ we have
    \[
        \frac{\| \Y-\X \btahat \|_{2}^{2}}{n}+\lambda\| \btahat \|_{1} \leq \frac{ \| \Y-\X \bta^0\|_{2}^{2}}{n}+\lambda \| \bta^0\|_{1}
    \]
    We now replace $\Y$ using equation \eqref{eqn: linear model (matrix)}:
    \begin{align*}
                 & \frac{\|\Y  - \X \btahat \|_{2}^{2}}{n} + \lambda \| \btahat\|_{1} \leq \frac{\| \Y  - \X \bta^0 \|_2^2}{n}+\lambda\|\bta^{0}\|_{1}                                                     \\
        \implies &
        \frac{\| (\X \bta^0 + \eps) - \X \btahat \|_{2}^{2}}{n}+\lambda\| \btahat \|_{1} \leq \frac{ \| (\X \bta^0 + \eps)-\X \bta^0\|_{2}^{2}}{n}+\lambda \| \bta^0\|_{1}                                 \\
        \implies &
        \frac{\langle \X ( \bta^0 - \btahat) + \eps, \X ( \bta^0 - \btahat) + \eps \rangle}{n}+\lambda \| \btahat \|_{1} \leq \frac{ \| \eps \|_{2}^{2}}{n}+\lambda \| \bta^0\|_{1}                        \\
        \implies &
        \frac{\| \X (\bta^0 - \btahat) \| _2^2 + \| \eps \|_2^2 + 2 \langle \X ( \bta^0 - \btahat),  \eps \rangle}{n}+\lambda \| \btahat \|_{1} \leq \frac{ \| \eps \|_{2}^{2}}{n}+\lambda \| \bta^0\|_{1} \\
        \implies &
        \frac{\| \X (\btahat - \bta^0) \| _2^2}{n} + \lambda \| \btahat \|_{1} \leq \frac{2 \langle \X ( \btahat - \bta^0),  \eps \rangle}{n} + \lambda \| \bta^0\|_{1}                                    \\
        \implies &
        \frac{\| \X (\btahat - \bta^0) \| _2^2}{n} + \lambda \| \btahat \|_{1} \leq \frac{2 \eps^T \X ( \btahat - \bta^0)}{n} + \lambda \| \bta^0\|_{1}                                                    \\
    \end{align*}
    This completes the proof.
\end{proof}
In the case of quadratic loss, we call the term
\[
    \frac{2 \eps^T \X ( \btahat - \bta^0)}{n}
\]
the empirical process. It depends on the measurement error, however we can easily bound it from above by replacing each component of \(\eps^T \X\) by the maximum over \(j = 1, \ldots, p\). We get \\
\[
    \frac{2 \eps^T \X ( \btahat - \bta^0)}{n} \leq \left( \max_{1 \leq j \leq p} 2 | \eps^T \X^{(j)}|\right) \| \btahat - \bta^0 \|_1
\]
Now let us define \(\T\) as follows:
\[
    \T := \left\{ \max _{1 \leq j \leq p} 2 \frac{\left| \varepsilon^{T} \X^{(j)}\right|}{n} \leq \lambda_{0}\right\}
\]
And in order to account for the random part of the problem, we assume that \(\lambda \geq 2 \lambda_0\). \\
We also define the Gram matrix, scaled by \(1/n\), as follows:
\[
    \Sgmhat := \frac{\X^T \X}{n}
\]
and we write its diagonal elements as
\[
    \sgmhat_j := \Sgmhat_{jj}, \quad j = 1, \ldots, p
\]
\(\T\) gives us a useful upper bound if we can find a value of \(\lambda_0\) such that \(\mathscr{T}\) has probability close to 1. In Lemma \ref{lemma: value for lambda_0}, will find such an upper bound and show that \(\T\) has indeed probability close to 1.

\begin{lemma}
    \label{lemma: value for lambda_0}
    Assume \(\forall \; j = 1, \ldots, p, \; \sgmhat_j^2 = 1 \) and for all $t > 0$ and let
    \[
        \lambda_{0}:=2 \sgm \sqrt{\frac{t^{2}+2 \log p}{n}}
    \]
    we have
    \[
        \mathbb{P}(\mathscr{T}) \geq 1-2 \exp \left[ \frac{-t^{2}}{2} \right]
    \]
\end{lemma}
\begin{proof}
    We define
    \[
        V_{j} := \frac{\varepsilon^{T} \X^{(j)}}{\sqrt{n \sgm^{2}}}
    \]
    Then we have
    \begin{align}
        \nonumber
        \mathbb{P}(\mathscr{T})
         & = \P \left( \max _{1 \leq j \leq p} 2 \frac{\left| \varepsilon^{T} \X^{(j)}\right|}{n} \leq 2 \sgm \sqrt{\frac{t^{2}+2 \log p}{n}}  \right) \\
        \nonumber
         & =
        \P \left( \max _{1 \leq j \leq p} \left| \frac{\varepsilon^{T} \X^{(j)}}{\sqrt{n \sgm^{2}}} \right| \leq \sqrt{t^{2}+2 \log p}  \right)        \\
        \nonumber
         & =
        \P \left( \max _{1 \leq j \leq p} | V_{j} | \leq \sqrt{t^{2}+2 \log p}  \right)                                                                \\
        \nonumber
         & = 1 - \P \left( \max _{1 \leq j \leq p} | V_{j} | > \sqrt{t^{2}+2 \log p}  \right)                                                          \\
        \nonumber
         & = 1 - \P \left( \bigcup_{j = 1}^{p} \left\{ | V_{j} | > \sqrt{t^{2}+2 \log p}  \right\} \right)                                             \\
        \nonumber
         & \geq 1 - \sum_{j = 1}^{p} \P \left( | V_{j} | > \sqrt{t^{2}+2 \log p}  \right)                                                              \\
        \label{eqn: proof lemma 6.2.}
         & \geq 1 - p \; \P \left( | V_{j} | > \sqrt{t^{2}+2 \log p}  \right)
    \end{align}
    Now, let us define $\zeta := \sqrt{t^{2}+2 \log p}$. Since $V_j$ is $\mathscr{N} (0,1)$-distributed and $\zeta > 0$, it follows that

    \begin{align*}
        \mathbb{P}(V_j > \zeta) & = \frac{1}{\sqrt{2\pi}} \int_\zeta^{\infty} e^{-y^2/2} dy                   \\
                                & < \frac{1}{\sqrt{2\pi}} \int_\zeta^{\infty} \frac{y}{\zeta} e^{-y^2 / 2} dy \\
                                & = \frac{1}{\zeta \sqrt{2\pi}} \int_\zeta^{\infty} y e^{-y^2 / 2} dy         \\
                                & = \frac{1}{\zeta\sqrt{2\pi}} e^{-\zeta^2/2}                                 \\
    \end{align*}
    We note that $p \geq 2 \implies \zeta \sqrt{2 \pi} \geq 1$ therefore
    \[
        \mathbb{P}(V_j > \zeta) <  e^{-\zeta^2/2}
    \]
    Moreover by symmetry of the $\mathscr{N} (0,1)$ distribution,

    \begin{align*}
        \mathbb{P} (|V_j| > \zeta) & = 2 \mathbb{P} (V_j > \zeta) \\
                                   & < 2 e^{-\zeta^2/2}           \\
    \end{align*}
    Inserting this result into \eqref{eqn: proof lemma 6.2.} we obtain

    \begin{align*}
        \mathbb{P}(\mathscr{T}) & \geq 1 - p \; \P \left( | V_{j} | > \sqrt{t^{2}+2 \log p}  \right) \\
                                & \geq 1 - p \; \frac{2}{p} \exp \left[ \frac{- t^{2}}{2} \right]    \\
                                & = 1 - 2 \exp \left[ \frac{- t^{2}}{2} \right]                      \\
    \end{align*}
\end{proof}
We extend lemma \ref{lemma: value for lambda_0} with the following corollary, which gives us an upper bound for the empirical process, taking the value of \(\sgm\) into account.
\begin{corollary}[Consistency of the LASSO] % Corollary 6.1
    Assume $\sgmhat^2_j = 1$ for all \(j = 1, \ldots, p\). For some \(t > 0\), let the regularization parameter be:
    \[
        \lambda = 4 \sgmhat \sqrt{\frac{t^2 + 2 \log p}{n}}
    \]
    where $\sgmhat$ is an estimator of $\sgm$. \\
    Then with probability at least $1 - \alpha$, where
    \[
        \alpha := 2 \exp \left[ \frac{- t^{2}}{2} \right] + \P (\{\sgmhat \leq \sgm\})
    \]
    we have
    \[
        2 \frac{\| \X (\btahat - \bta^0)\|_2^2}{n} \leq 3 \lambda \| \bta^0 \|_1
    \]
\end{corollary}
\begin{proof}
    Recall that we defined
    \[
        \mathscr{T} := \left\{ \max _{1 \leq j \leq p} 2 \frac{\left| \varepsilon^{T} \X^{(j)}\right|}{n} \leq \lambda_{0}\right\}
    \]
    and lemma \ref{lemma: value for lambda_0}, we know that
    \[
        \lambda_{0} = 2 \sgm \sqrt{\frac{t^{2}+2 \log p}{n}} \implies \mathbb{P}(\mathscr{T}) \geq 1-2 \exp \left[ \frac{-t^{2}}{2} \right]
    \]
    So if \(\sgmhat > \sgm\), replacing the latter by the former will result in a weaker statement, which therefore still holds, \ie
    \[
        \lambda_{0} = 2 \sgmhat \sqrt{\frac{t^{2}+2 \log p}{n}} \implies \mathbb{P}(\mathscr{T}) \geq 1-2 \exp \left[ \frac{-t^{2}}{2} \right]
    \]
    Now if we are on \(\T\), starting with the basic inequality \ref{lemma: basic inequality}:
    \begin{align*}
                 & \frac{\| \X (\btahat-\bta^{0})\|_{2}^{2}}{n} + 2\lambda_0 \|\btahat\|_{1} \leq 2 \frac{\eps^{T} \X (\btahat-\bta^{0})}{n} + 2\lambda_0 \| \bta^{0} \|_{1}                                             \\
        \implies & \frac{\| \X (\btahat-\bta^{0})\|_{2}^{2}}{n} \leq \underbrace{2 \frac{\eps^{T} \X (\btahat - \bta^{0})}{n}}_{\text{bounded on } \T} + 2 \lambda_0 \left( \| \bta^{0} \|_{1} - \|\btahat\|_{1} \right) \\
        \implies & \frac{\| \X (\btahat-\bta^{0})\|_{2}^{2}}{n} \leq \lambda_0 ( \underbrace{\|\btahat - \bta^{0} \|_1 - \|\btahat\|_{1}}_{\leq \| \bta^{0} \|_{1}} + 2 \| \bta^{0} \|_{1} )                             \\
        \implies & \frac{\| \X (\btahat-\bta^{0})\|_{2}^{2}}{n} \leq 3 \lambda_0 \| \bta^{0} \|_{1}
    \end{align*}
    Therefore with \(\lambda = 2 \lambda_0 = 4 \sgmhat \sqrt{\frac{t^2 + 2 \log p}{n}}\), we obtain the desired result. This occurs if we are on \( \T \) and \(\sgmhat > \sgm\), which means that it doesn't occur with probability
    \begin{align*}
         & \P(\neg \T \cup \{ \sgmhat \leq \sgm \})                                                          \\
         & = \P(\neg \T) + \P(\{ \sgmhat \leq \sgm \}) - \P(\neg \T \cap \{ \sgmhat \leq \sgm \})            \\
         & \leq \P(\neg \T) + \P(\{ \sgmhat \leq \sgm \})                                                    \\
         & = 1 - \P(\T) + \P(\{ \sgmhat \leq \sgm \})                                                        \\
         & \leq 1 - \left( 1 - 2 \exp \left[ \frac{- t^{2}}{2} \right] \right) + \P(\{ \sgmhat \leq \sgm \}) \\
         & = 2 \exp \left[ \frac{- t^{2}}{2} \right] + \P(\{ \sgmhat \leq \sgm \})
    \end{align*}
    which concludes the proof.
\end{proof}
Now we want to go one step further and we need to introduce some supplementary notation. We write \(\bta_{j,S}\) for the vector \(\bta\), where we keep only the components which belong to \(S\) and set the other ones to 0. That is:
\[
    \bta_{j,S} := \bta_{j} \mathbf{1}_{j \in S}
\]
with \(\mathbf{1}\) being the indicator function. \\
Naturally, since any given index either belongs to a set or to its complement, we have that
\[
    \bta = \bta_{j,S} + \bta_{j,S^C}
\]
We now have the necessary notation so we proceed with the lemma.
\begin{lemma} % Lemma 6.3.
    \label{lemma: lemma 6.3.}
    We have on $\T$, with $\lambda \geq 2 \lambda_{0}$,
    \begin{equation*}
        2\frac{ \|\X (\btahat-\bta^{0} ) \|_{2}^{2}}{n} + \lambda \|\btahat_{S_{0}^{c}} \|_{1} \leq 3 \lambda \|\btahat_{S_{0}}-\bta_{S_{0}}^{0} \|_{1}
    \end{equation*}
\end{lemma}
\begin{proof}
    We start with the Basic Inequality
    \[
        \frac{\| \X (\btahat-\bta^{0})\|_{2}^{2}}{n} + \lambda\|\btahat\|_{1} \leq 2 \frac{\varepsilon^{T} \X (\btahat-\bta^{0})}{n}+\lambda\|\bta^{0}\|_{1}
    \]
    Now since we are on $\mathscr{T}$ and since we assumed that \( \lambda \geq 2 \lambda_0 \)
    \[
        \frac{\| \X (\btahat-\bta^{0})\|_{2}^{2}}{n} + \lambda\|\btahat\|_{1} \leq \lambda_0 \|\btahat-\bta^{0}\|_1 + \lambda\|\bta^{0}\|_{1}
    \]
    \[
        2 \frac{\| \X (\btahat-\bta^{0})\|_{2}^{2}}{n}+2 \lambda\|\btahat\|_{1} \leq \lambda \|\btahat-\bta^{0} \|_{1}+2 \lambda \|\bta^{0} \|_{1}
    \]
    Let $\bta_{j, S}:=\bta_{j} 1\{j \in S\}$. We use the triangle inequality on the left hand side
    \begin{align*}
        \|\btahat\|_{1}
         & = \|\btahat_{S_{0}} \|_{1} + \|\btahat_{S_{0}^{c}} \|_{1}                                                \\
         & = \|\bta_{S_{0}}^{0} - \bta_{S_{0}}^{0} + \btahat_{S_{0}} \|_{1} + \|\btahat_{S_{0}^{c}} \|_{1}          \\
         & \geq \|\bta_{S_{0}}^{0} \|_{1} - \|\btahat_{S_{0}}-\bta_{S_{0}}^{0} \|_{1}+ \|\btahat_{S_{0}^{c}} \|_{1}
    \end{align*}
    whereas on the right hand side
    \begin{align*}
        \|\btahat-\bta^{0} \|_{1}
         & =  \| (\btahat_{S_{0}} + \btahat_{S_{0}^{c}}) - (\bta_{S_{0}}^{0} + \underbrace{\bta_{S_{0}^{c}}^0)}_{=0} \|_{1} \\
         & =  \| \btahat_{S_{0}}-\bta_{S_{0}}^{0} \|_{1} + \|\btahat_{S_{0}^{c}} \|_{1}
    \end{align*}
    Injecting these two results, we get that
    \begin{align*}
                 & 2 \frac{\| \X (\btahat-\bta^{0})\|_{2}^{2}}{n} + 2 \lambda\|\btahat\|_{1} \leq \lambda \|\btahat-\bta^{0} \|_{1}+2 \lambda \|\bta^{0} \|_{1}                                                                                                \\
        \implies & 2 \frac{\| \X (\btahat-\bta^{0})\|_{2}^{2}}{n} + 2 \lambda \left( \|\bta_{S_{0}}^{0} \|_{1} - \|\btahat_{S_{0}}-\bta_{S_{0}}^{0} \|_{1}+ \|\btahat_{S_{0}^{c}} \|_{1} \right)                                                               \\
                 & \leq \lambda \left( \| \btahat_{S_{0}}-\bta_{S_{0}}^{0} \|_{1} + \|\btahat_{S_{0}^{c}} \|_{1} \right) + 2 \lambda \|\bta^{0} \|_{1}                                                                                                         \\
        \implies & 2 \frac{\| \X (\btahat-\bta^{0})\|_{2}^{2}}{n} + 2 \lambda \|\underbrace{\bta_{S_{0}}^{0}}_{= \bta^0} \|_{1} + \lambda \|\btahat_{S_{0}^{c}} \|_{1} \leq 3 \lambda \| \btahat_{S_{0}}-\bta_{S_{0}}^{0} \|_{1} + 2 \lambda \|\bta^{0} \|_{1} \\
        \implies & 2\frac{ \|\X (\btahat-\bta^{0} ) \|_{2}^{2}}{n} + \lambda \|\btahat_{S_{0}^{c}} \|_{1} \leq 3 \lambda \|\btahat_{S_{0}}-\bta_{S_{0}}^{0} \|_{1}
    \end{align*}
\end{proof}
We now introduce an important assumption that we will require in order to deduce several results in the future. Let us first define it, then we will discuss its importance.
\begin{definition}[Compatibility condition]
    We say that the compatibility condition is met for the set $S_{0}$, if for some $\phi_{0}>0$, and for all $\bta$ satisfying $ \|\bta_{S_{0}^{c}} \|_{1} \leq 3 \|\bta_{S_{0}} \|_{1}$, it holds that
    \begin{equation}
        \left\| \bta_{S_{0}} \right\|_{1}^{2} \leq \left(\bta^{T} \Sgmhat \bta\right) \frac{s_{0}}{\phi_{0}^{2}}
    \end{equation}
\end{definition}
The compatibility condition gives us a permission to cross the bridge between the \(\ell_1\) world and the \(\ell_2\) world. That bridge is obtained thanks the the Cauchy-Scharz inequality, and it is:
\[
    \left\|\ \btahat_{S_{0}} - \bta_{S_{0}}^{0} \right\|_{1} \leq \sqrt{s_{0}}\left\|\btahat_{S_{0}} - \bta_{S_{0}}^{0} \right\|_{2}
\]
Moreover we have that
\begin{align*}
    \frac{\left\|\X\left(\btahat - \bta^{0}\right)\right\|_{2}^{2}}{n}
     & = \frac{\left[ \X \left( \btahat - \bta^{0}\right) \right]^T \left[\X \left(\btahat - \bta^{0}\right) \right]}{n} \\
     & = \left(\btahat - \bta^{0}\right)^{T} \frac{ \X^T \X }{n} \left(\btahat - \bta^{0}\right)                         \\
     & = \left(\btahat - \bta^{0}\right)^{T} \Sgmhat \left(\btahat - \bta^{0}\right)
\end{align*}
And we hope that for some constant \(\phi_0 > 0\):
\[
    \left\| \btahat_{S_0} - \bta_{S_0}^{0} \right\|_{2}^{2} \leq \frac{\left(\btahat - \bta^{0}\right)^{T} \Sgmhat \left(\btahat - \bta^{0}\right)}{\phi_0^2}
\]
however, this is not the case for all \(\bta\). But lemma \ref{lemma: lemma 6.3.} tells us that
\[
    \left\|\btahat_{S_{0}^{c}}\right\|_{1} \leq 3\left\|\btahat_{S_{0}}-\bta_{S_{0}}^{0}\right\|_{1}
\]
as long as we are on \(\T\). \\
We may now use the compatibility condition to get the following theorem.
\begin{theorem} % Theorem 6.1.
    \label{theorem: theorem 6.1}
    Suppose the compatibility condition holds for $S_{0}$. Then on $\mathscr{T}$, we have for $\lambda \geq 2 \lambda_{0}$,
    \[
        \frac{\|\X(\btahat-\bta^{0})\|_{2}^{2}}{n}+\lambda\|\btahat-\bta^{0}\|_{1} \leq 4 \lambda^{2} \frac{s_{0}}{\phi_{0}^{2}}
    \]
\end{theorem}
\begin{proof}
    Using Lemma \ref{lemma: lemma 6.3.} we have that
    \begin{align*}
         & \quad 2 \frac{\|\X(\btahat-\bta^{0})\|_{2}^{2}}{n} + \lambda\|\btahat-\bta^{0}\|_{1}                                                                                                      \\
         & = 2 \frac{\|\X(\btahat-\bta^{0})\|_{2}^{2}}{n} + \lambda \| \btahat_{S_0} + \btahat_{S_{0}^{c}} - \bta^{0}_{S_0} - \underbrace{\bta^{0}_{S_0^c}}_{=0} \|_{1}                              \\
         & = 2 \frac{\|\X(\btahat-\bta^{0})\|_{2}^{2}}{n} + \lambda \| \btahat_{S_0} - \bta^{0}_{S_0} \|_{1} + \lambda \| \btahat_{S_{0}^{c}} \|_1 \quad \textit{(by lemma \ref{lemma: lemma 6.3.})} \\
         & \leq 4 \lambda \|\btahat_{S_{0}} - \bta_{S_{0}}^{0} \|_{1}                                                                                                                                \\
         & = 4 \lambda \sqrt{\left( \btahat_{S_{0}} - \bta_{S_{0}}^{0} \right)^{T} \Sgmhat \left( \btahat_{S_{0}} - \bta_{S_{0}}^{0}  \right) s_{0} / \phi_{0}^{2}}                                  \\
         & \leq \sqrt{\left( \btahat_{S_{0}} - \bta_{S_{0}}^{0} \right)^{T} \X^T \X \left( \btahat_{S_{0}} - \bta_{S_{0}}^{0}  \right)} \frac{4 \lambda \sqrt{s_{0}}}{\phi_{0} \sqrt{n}}             \\
         & \leq \| \X ( \btahat_{S_{0}} - \bta_{S_{0}}^{0} ) \|_2 \frac{4 \lambda \sqrt{s_{0}}}{\phi_{0} \sqrt{n}}                                                                                   \\
         & \leq \| \X ( \btahat_{S_{0}} - \bta_{S_{0}}^{0} ) \|^2_2 + \frac{4 \lambda^2 s_{0}}{\phi^2_{0} n}                                                                                         \\
    \end{align*}
    Where the last inequality follows from $4uv \leq u^2 + 4v^2$.
\end{proof}
We proceed with the next section, which removes the condition of having null expected value for the random part of our model.

\section{Linear approximation of the truth}

In this section, we take into account the fact that the expected value of \(\Y\) may not be 0. Several points run in a similar fashion to section \ref{section: assuming truth is linear} but carry extra terms, which account for that difference. \\
From now on, we assume the expected value of \(\Y\) to be \(\mathbb{E} [\mathbf{Y}] :=\f^{0}\). The basic inequality still holds, but carries extra terms
\begin{lemma}[Basic Inequality for non-zero expected value]
    For all \(\bta^* \in \R^p\) we have
    \begin{equation}
        \frac{\|\X \btahat - \f^0 \|_{2}^{2}}{n} + \lambda\| \btahat \|_{1} \leq \frac{2 \eps^T \X (\btahat - \bta^*)}{n} + \lambda \| \bta^*\|_{1} + \frac{\|\X \bta^* - \f^0\|_{2}^{2}}{n}
    \end{equation}
\end{lemma}
\begin{proof}
    By definition of \(\btahat\), we have that
    \[
        \forall \bta \quad \frac{\| \Y -\X \btahat \|_{2}^{2}}{n} + \lambda \| \btahat \|_{1} \leq \frac{\|\Y-\X \bta \|_{2}^{2}}{n}+\lambda \| \bta \|_{1}
    \]
    In particular for $\bta = \bta^*$ we have
    \[
        \forall \bta^* \quad \frac{\| \Y -\X \btahat \|_{2}^{2}}{n} + \lambda \| \btahat \|_{1} \leq \frac{\|\Y-\X \bta^* \|_{2}^{2}}{n}+\lambda \| \bta^* \|_{1}
    \]
    Since $\Y = \f^0 + \eps$:
    \begin{align*}
                 & \frac{\|\Y  - \X \btahat \|_{2}^{2}}{n} + \lambda \| \btahat\|_{1} \leq \frac{\| \Y  - \X \bta^* \|_2^2}{n}+\lambda\|\bta^{*}\|_{1}                                       \\
        \implies &
        \frac{\| (\f^0 - \X \btahat ) + \eps \|_{2}^{2}}{n} + \lambda\| \btahat \|_{1} \leq \frac{ \| (\f^0 - \X \bta^*) + \eps \|_{2}^{2}}{n}+\lambda \| \bta^*\|_{1}                       \\
        \implies &
        \frac{\langle (\f^0 - \X \btahat ) + \eps, (\f^0 - \X \btahat ) + \eps \rangle}{n} + \lambda\| \btahat \|_{1}                                                                        \\
                 & \leq \frac{ \langle (\f^0 - \X \bta^*) + \eps, (\f^0 - \X \bta^*) + \eps \rangle}{n}+\lambda \| \bta^*\|_{1}                                                              \\
        \implies &
        \frac{\|\f^0 - \X \btahat \|_{2}^{2} + \| \eps \|_{2}^{2} + 2\langle \f^0 - \X \btahat, \eps \rangle}{n} + \lambda\| \btahat \|_{1}                                                  \\
                 & \leq \frac{\|\f^0 - \X \bta^* \|_{2}^{2} + \| \eps \|_{2}^{2} + 2\langle \f^0 - \X \bta^*, \eps \rangle}{n}+\lambda \| \bta^*\|_{1}                                       \\
        \implies &
        \frac{\|\X \btahat - \f^0 \|_{2}^{2}}{n} + \lambda\| \btahat \|_{1} \leq \frac{2 \eps^T \X (\btahat - \bta^*)}{n} + \lambda \| \bta^*\|_{1} + \frac{\|\X \bta^* - \f^0\|_{2}^{2}}{n} \\
    \end{align*}
\end{proof}
We can also adapt lemma \ref{lemma: lemma 6.3.} for non-zero expected value.
\begin{lemma}
    We have on \(\mathscr{T}\), with \(\lambda \geq 4 \lambda_{0}\):
    \begin{equation}
        \label{eqn: new version of lemma 6.3.}
        \frac{4 \|\X \btahat-\f^0\|_{2}^{2}}{n} + 3 \lambda \| \btahat_{S^{c}_{*}} \|_{1} \leq 5 \lambda \| \btahat_{S_{*}} - \bta_{S_{*}}^{*} \|_{1} + \frac{4 \|\X \bta^{*} - \f^{0} \|_{2}^{2}}{n}
    \end{equation}
    where $S_* := \{ j: \bta_j^* \neq 0 \}$.
\end{lemma}
\begin{proof}
    We start with the Basic Inequality
    $$
        \frac{\|\X \btahat - \f^0 \|_{2}^{2}}{n} + \lambda\| \btahat \|_{1} \leq  \frac{2 \eps^T \X (\btahat - \bta^*)}{n} + \lambda \| \bta^*\|_{1} + \frac{\|\X \bta^* - \f^0\|_{2}^{2}}{n}
    $$
    Now since we are on $\mathscr{T}$ and since $4 \lambda_0 \leq \lambda$
    \begin{align*}
                 & \frac{\|\X \btahat - \f^0 \|_{2}^{2}}{n} + \lambda\| \btahat \|_{1} \leq  \frac{2 \eps^T \X (\btahat - \bta^*)}{n} + \lambda \| \bta^*\|_{1} + \frac{\|\X \bta^* - \f^0\|_{2}^{2}}{n} \\
        \implies &
        4 \frac{\| \X \btahat - \f^0 \|_{2}^{2}}{n} + 4 \lambda\| \btahat \|_{1} \leq \lambda \| \btahat - \bta^* \|_{1} + 4 \lambda \| \bta^*\|_{1} + 4\frac{\|\X \bta^* - \f^0\|_{2}^{2}}{n}           \\
    \end{align*}
    We use the triangle inequality on the left hand side
    \begin{align*}
        \|\btahat\|_{1}
         & = \|\btahat_{S_{*}} \|_{1} + \|\btahat_{S_{*}^{c}} \|_{1}                                                \\
         & = \|\bta_{S_{*}}^{*} - \bta_{S_{*}}^{*} + \btahat_{S_{*}} \|_{1} + \|\btahat_{S_{*}^{c}} \|_{1}          \\
         & \geq \|\bta_{S_{*}}^{*} \|_{1} - \|\btahat_{S_{*}}-\bta_{S_{*}}^{*} \|_{1}+ \|\btahat_{S_{*}^{c}} \|_{1}
    \end{align*}
    whereas on the right hand side
    \begin{align*}
        \|\btahat-\bta^{*} \|_{1}
         & =  \| (\btahat_{S_{*}} + \btahat_{S_{*}^{c}}) - (\bta_{S_{*}}^{*} + \underbrace{\bta_{S_{*}^{c}}^*}_{=0}) \|_{1} \\
         & =  \| \btahat_{S_{*}}-\bta_{S_{*}}^{*} \|_{1} + \|\btahat_{S_{*}^{c}} \|_{1}
    \end{align*}
    Injecting these two results, we get that
    \begin{align*}
                 & 4 \frac{\| \X \btahat - \f^0 \|_{2}^{2}}{n} + 4 \lambda\| \btahat \|_{1} \leq \lambda \| \btahat - \bta^* \|_{1} + 4 \lambda \| \bta^*\|_{1} + 4\frac{\|\X \bta^* - \f^0\|_{2}^{2}}{n} \\
        \implies &
        4 \frac{\| \X \btahat - \f^0 \|_{2}^{2}}{n} + 4 \lambda \left( \|\bta_{S_{*}}^{*} \|_{1} - \|\btahat_{S_{*}}-\bta_{S_{*}}^{*} \|_{1}+ \|\btahat_{S_{*}^{c}} \|_{1} \right)                        \\
                 & \leq \lambda \left( \| \btahat_{S_{*}}-\bta_{S_{*}}^{*} \|_{1} + \|\btahat_{S_{*}^{c}} \|_{1} \right) + 4 \lambda \| \bta^*\|_{1} + 4\frac{\|\X \bta^* - \f^0\|_{2}^{2}}{n}            \\
        \implies &
        4 \frac{\| \X \btahat - \f^0 \|_{2}^{2}}{n} + 4 \lambda \| \underbrace{\bta_{S_{*}}^{*}}_{= \bta^{*}} \|_{1} + 3 \lambda\|\btahat_{S_{*}^{c}} \|_{1}                                              \\
                 & \leq 5 \lambda \| \btahat_{S_{*}}-\bta_{S_{*}}^{*} \|_{1} + 4 \lambda \| \bta^*\|_{1} + 4\frac{\|\X \bta^* - \f^0\|_{2}^{2}}{n}                                                        \\
        \implies &
        4 \frac{\| \X \btahat - \f^0 \|_{2}^{2}}{n} + 3 \lambda\|\btahat_{S_{*}^{c}} \|_{1} \leq 5 \lambda \| \btahat_{S_{*}}-\bta_{S_{*}}^{*} \|_{1} + 4\frac{\|\X \bta^* - \f^0\|_{2}^{2}}{n}           \\
    \end{align*}
\end{proof}
The compatibility condition in its form for non-zero expected value is given as follows.
\begin{definition}[Compatibility condition for general sets]
    We say that the compatibility condition holds for the set $S$, if for some constant $\phi(S)>0,$ and for all $\bta,$ with $\left\|\bta_{S^{c}}\right\|_{1} \leq$ $3\left\|\bta_{S}\right\|_{1},$ one has
    \[
        \left\|\bta_{S}\right\|_{1}^{2} \leq\left(\bta^{T} \sgmhat \bta\right)\frac{|S|}{\phi^{2}(S)}
    \]
\end{definition}
We use the notation $\mathscr{S}$ for the collection of sets $S$ for which the compatibility condition holds. Then, let us define the oracle, which is the best possible value for the estimator \(\bta\).
\begin{definition}[The oracle]
    We define the oracle $\bta^{*}$ as
    \[
        \bta^{*} = \arg \min_{\bta: S_{\bta} \in \mathscr{S}} \left\{ \frac{ \left\| \X \bta - \f^{0} \right\|_{2}^{2}}{n} + \frac{4 \lambda^{2} s_{\bta}}{\phi^{2}\left(S_{\bta}\right)}\right\}
    \]
    where $S_{\bta}:=\left\{j: \bta_{j} \neq 0\right\}$, $s_{\bta}:=\left|S_{\bta}\right|$ denotes the cardinality of $S_{\bta}$ and the factor 4 in the right hand side comes from choosing $\lambda \geq \lambda_0$.
\end{definition}
We define for simplicity
\begin{align*}
    S_*    & := S_{\bta^*} \\
    s_*    & := |S_*|      \\
    \phi_* & := \phi(S_*)  \\
\end{align*}
Now building up from the definition of the oracle, we continue with the main result of this section.
\begin{theorem}
    \label{theorem: theorem 6.2}
    Assume that \(\sgmhat_{j}^{2}=1\) for all \(j\) and that the compatibility condition holds for all \(S \in \mathscr{S}\), where \(\Sgmhat\) is normalized. For some \(t>0\), we let the regularization parameter be
    \[
        \lambda=8 \sgmhat \sqrt{\frac{t^{2} + 2 \log p}{n}}
    \]
    where \(\sgmhat^{2}\) is an estimator of the noise variance \(\sgm^{2}\). Then with probability at least \(1-\alpha\), where
    \[
        \alpha := 2 \exp \left[ \frac{-t^{2}}{2} \right] + \P(\sgmhat \leq \sgm)
    \]
    we have
    \[
        \frac{2\left\|\X \btahat-\f^{0}\right\|_{2}^{2}}{n} +  \lambda\left\|\btahat-\bta^{*}\right\|_{1} \leq \frac{6 \left\|\X \bta^{*} - \f^{0}\right\|_{2}^{2}}{n} + \frac{24 \lambda^{2} s_{*}}{\phi_{*}^{2}}
    \]
\end{theorem}
\begin{proof}
    We will distinguish between two cases: first, when \(\lambda\left\|\btahat_{S_{*}}-\bta_{S_{*}}^{*}\right\|_{1}\) is at least \(\frac{\left\|\X \bta^{*}-\f^{0}\right\|_{2}^{2}}{n}\), then when it is smaller.
    \begin{enumerate}
        \item On $\mathscr{T}$, consider the case when
              \[
                  \lambda\left\|\btahat_{S_{*}}-\bta_{S_{*}}^{*}\right\|_{1} \geq \frac{\left\|\X \bta^{*}-\f^{0}\right\|_{2}^{2}}{n}
              \]
              we have
              \[
                  \frac{4\left\|\X \btahat-\f^{0}\right\|_{2}^{2}}{n}+3 \lambda\left\|\btahat_{S_{*}^{c}}\right\|_{1} \leq 9 \lambda\left\|\btahat_{S_{*}}-\bta_{S_{*}}^{*}\right\|_{1},
              \]
              therefore by the assumption relative to the current case, this above inequality becomes:
              \begin{align*}
                   & \frac{4\left\|\X \btahat-\f^{0}\right\|_{2}^{2}}{n} + 3 \lambda\left\|\btahat - \bta^{*}\right\|_{1}                                                          \\
                   & \leq 12 \lambda\left\|\btahat_{S_{*}}-\bta_{S_{*}}^{*}\right\|_{1}                                                                                            \\
                   & \leq \frac{12 \lambda \sqrt{s_{*}}\left\|\X\left(\btahat-\bta^{*}\right)\right\|_{2}}{\sqrt{n} \phi_{*}}                                                      \\
                   & \leq \frac{24 \lambda^{2} s_{*}}{\phi_{*}^{2}} + \frac{2\left\|\X \btahat-\f^{0}\right\|_{2}^{2}}{n} + \frac{6\left\|\X \bta^{*} - \f^{0}\right\|_{2}^{2}}{n}
              \end{align*}
              Where, we used that
              \[
                  0 \leq 2(3u + v)^2                             \implies  12 u v \leq 18 u^{2}+2 v^{2}
              \]
              and
              \[
                  0 \leq 6(u + v)^2                             \implies  12 u v \leq 6 u^{2}+6 v^{2}
              \]
              hence
              \[
                  \frac{2\left\|\X \btahat-\f^{0}\right\|_{2}^{2}}{n} + 3 \lambda\left\|\btahat-\bta^{*}\right\|_{1} \leq \frac{6\left\|\X \bta^{*}-\f^{0}\right\|_{2}^{2}}{n} + \frac{24 \lambda^{2} s_{*}}{\phi_{*}^{2}}
              \]
        \item We now consider the other case, still on $\mathscr{T}$. This time we work in the following setting:
              \[
                  \lambda\left\|\btahat_{S_{*}}-\bta_{S_{*}}^{*}\right\|_{1} < \frac{\left\|\X \bta^{*}-\f^{0}\right\|_{2}^{2}}{n}
              \]
              we get
              \[
                  \frac{4\left\|\X \btahat-\f^{0}\right\|_{2}^{2}}{n} + 3 \lambda\left\|\btahat_{S_{*}^{c}}\right\|_{1} \leq \frac{9\left\|\X \bta^{*}-\f^{0}\right\|_{2}^{2}}{n}
              \]
              and hence
              \[
                  \frac{4\left\|\X \btahat-\f^{0}\right\|_{2}^{2}}{n} + 3 \lambda\left\|\btahat-\bta^{*}\right\|_{1} \leq \frac{12\left\|\X \bta^{*}-\f^{0}\right\|_{2}^{2}}{n}
              \]
    \end{enumerate}
\end{proof}
Theorem \ref{theorem: theorem 6.2}, as well as this whole section give us useful results generalizing those from section \ref{section: assuming truth is linear} as they account for the expected value of the random part of the model being \(\f^0\), which is possibly not zero.
