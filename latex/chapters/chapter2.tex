\chapter{Classical theory of Linear Regression}

Part of this chapter follows \textit{Introduction to Linear Regression Analysis}, fifth edition by Douglas C. Montgomery, Elizabeth A. Peck and G. Geoffrey Vining.

\section{Linear models}

We consider the setting of having a sample of $n$ observations
\[
    (\X_1, \Y_1), \ldots ,(\X_n, \Y_n)
\]
where $X_i \in \mathscr{X} \subseteq \R^p, \; i = 1, \ldots, n$ and $Y_i \in \mathscr{Y} \subseteq \R, \; i = 1, \ldots, n$.

\begin{definition}[The linear model]
    The relationship between an observation $\X_i \in \mathscr{X}$ and its outcome $\Y_i \in \mathscr{Y}$ can be established by a linear model, that is
    \begin{equation}
        i = 1, \ldots, n \qquad \Y_{i} = \sum_{j=1}^{p} \bta_{j} \X_{i}^{(j)} + \eps_{i}
    \end{equation}
    where \(\eps_1, \ldots, \eps_n\) are independent and identically distributed (i.i.d.). Moreover, \(\forall \, i = 1, \ldots, n, \) we have that \( \E [\eps_i] = 0\) and each \(\eps_i\) is independent of all of the \( X_j, \; j=1, \ldots, n \).
\end{definition}

Instead of seeing each observation individually we can deal with all of them together by expressing the linear model in matrix notation

\begin{equation}
    \label{eqn: linear model (matrix)}
    \Y = \X \bta + \eps
\end{equation}

\begin{definition}
    \begin{enumerate}[label=(\alph*)]
        \item $\X$ is called the \textbf{design matrix}. It has dimension $n \times p$. $\X$ consists of stacking the vectors relative to each observation inside of a matrix
              \[
                  X =
                  \begin{bmatrix}
                      \text{---} & X_1^T  & \text{---} \\
                                 & \vdots &            \\
                      \text{---} & X_n^T  & \text{---} \\
                  \end{bmatrix}
              \]
        \item $\bta$ is called the \textbf{parameter vector}. It has dimension $p \times 1$.
        \item $\eps$ is called the \textbf{error vector}. It has dimension $n \times 1$.
        \item $\Y$ is called the \textbf{response vector}. It has dimension $n \times 1$.
    \end{enumerate}
\end{definition}

% add some preparation here

\section{The least squares method}
\subsection{Estimation of the model parameters}

The least squares method consists of finding a vector minimizing a function. We call that function the objective function and we define it as follows.

\begin{definition}
    We define the \textbf{objective function} \( S \) as follows
    \begin{equation}
        \label{eqn: least squares objective function}
        S(\bta) := \| \Y-\X \bta \|_2^2 = \eps^{T} \eps
    \end{equation}
\end{definition}
We may rewrite the objective function as
\begin{align*}
    S(\bta)
     & = \| \Y-\X \bta \|_2^2                                          \\
     & = (\Y - \X \bta)^{T} (\Y - \X \bta)                             \\
     & = \Y^T \Y - \Y^T \X \bta - \bta^T \X^T \Y + \bta^T \X^T \X \bta \\
     & = \Y^T \Y - 2 \bta^T \X^T \Y + \bta^T \X^T \X \bta              \\
\end{align*}
As mentioned previsouly, the least squares method aims at finding the vector $\btahat$ minimizing \( S \), that is
\[
    \btahat := \arg \min_{\bta} S(\bta)
\]
We find \( \btahat \) by differentiating \( S \) with respect to $\bta$ and setting the result to 0.
\begin{align}
             & \frac{\partial}{\partial \bta} S(\bta) \Big|_{\bta = \btahat} = 0 \nonumber                                                        \\
    \implies & \frac{\partial}{\partial \bta} \left( \Y^T \Y - 2 \bta^T \X^T \Y + \bta^T \X^T \X \bta \right)\Big|_{\bta = \btahat} = 0 \nonumber \\
    \implies & - 2 \X^T \Y + 2 \X^T \X \btahat = 0 \nonumber                                                                                      \\
    \implies & \X^T \X \btahat = \X^T \Y \label{eqn: least-squares normal equations} \tag{LSNE}
\end{align}
where equation \eqref{eqn: least-squares normal equations} is called the least squares normal equations.

If we assume that \( \X^T \X \) is invertible, then \eqref{eqn: least-squares normal equations} yields that our least squares estimator \( \btahat \) is given by
\begin{equation}
    \label{eqn: least squares estimator}
    \btahat = (\X^T \X)^{-1} \X^T \Y
\end{equation}
Now, we can take a look at a few properties of our estimator, namely the expected value and the variance.
\begin{proposition}
    \(\btahat\) is unbiased, that is $\E [\btahat] = \bta$.
\end{proposition}
\begin{proof}
    \begin{align*}
        \E \left[ \btahat \right]
         & = \E \left[ \left(\X^{T} \X \right)^{-1} \X^{T} \Y \right]                                                \\
         & = \E \left[ \left(\X^{T} \X \right)^{-1} \X^{T}(\X \bta + \eps)\right]                                    \\
         & = \E \left[ \left(\X^{T} \X \right)^{-1} \X^{T} \X \bta + \left(\X^{T} \X\right)^{-1} \X^{T} \eps \right] \\
         & = \bta
    \end{align*}

\end{proof}
Moreover, we want to study the estimator's variance. For this purpose we will use the covariance matrix. Let \(U, V \in \R^p\), recall that the covariance matrix is defined as
\[
    \cov (U, V) := \E \left[ \left( U - \E (U) \right) \left( V - \E (V) \right)^T \right] \in \mathcal{M}_{p \times p}(\R)
\]
where \(\forall \, i, j = 1, \ldots, p, \; \cov (U, V)_{ij}\) is the covariance between \( U_i \) and \( V_j\). In the particular case \( U = V \), the diagonal of the covariance matrix is nothing else than the variance of \( U \), that is:
\[
    \var (U)_i = \cov (U, U)_{ii} \quad i = 1, \ldots, p
\]
\begin{proposition}
    For \( \, i, j = 1, \ldots, p\), we have that:
    \begin{enumerate}[label=(\roman*)]
        \item \(\cov (\btahat_i, \btahat_j) = \sgm^{2} \left[ \left( \X^T \X \right)^{-1} \right]_{ij}\)
        \item \(\var (\btahat_i) = \sgm^{2} \left[ \left( \X^T \X \right)^{-1} \right]_{ii}\)
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}[label=(\roman*)]

        \item One can note that\begin{align*}
                  \cov(\btahat, \btahat)
                   & = \var \left[ \underbrace{\left(\X^T \X\right)^{-1} \X^T}_{\text{constant}} \Y\right]                               \\
                   & = \left(\X^T \X\right)^{-1} \X^T \underbrace{\var(\Y)}_{= \sgm^2 I} \left[\left( \X^T \X \right)^{-1} \X^T\right]^T \\
                   & = \sgm^{2}\left(\X^T \X\right)^{-1} \X^T \X\left(\X^T \X\right)^{-1}                                                \\
                   & = \sgm^{2}\left(\X^T \X\right)^{-1}
              \end{align*}
        \item This is a direct consequence of the first point.
    \end{enumerate}
\end{proof}

Now that we confirmed that \( \btahat \) does indeed exhibit the above properties, we are interested in estimating the quality of our prediction. The residuals can help us do that.
\begin{definition}[Residuals]
    For a given set of observations \(\Y\), the \textbf{residuals} (or \textbf{vector of residuals}) is the difference between the prediction of our model and the observed value, that is
    \[
        X (\btahat - \bta) \in \R^n
    \]
\end{definition}
Building up from the residuals, we would like a measure that indicates how far our predictions are from the measurements. We will use the prediction error for that purpose.
\begin{definition}
    For a given set of observations \(\Y\), the \textbf{prediction error} (also called \textbf{residual sum of squares}) is the squared \(\ell^2\)-norm of the difference between the prediction of our model and the observed value, that is
    \[
        \prederr = \| \Y - \hat{\Y} \|_2^2 = \| \eps \|_2^2
    \]
\end{definition}
The prediction error can be rewritten as follows:
\begin{align*}
    \prederr
     & = \| \eps \|_2^2                                                                     \\
     & = \eps^T \eps                                                                        \\
     & = \left( \Y - \X \btahat \right)^T \left(\Y - \X \btahat  \right)                    \\
     & = \Y^T \Y - \btahat^T \X^T \Y - \Y^T \X \btahat + \btahat^T \X^T \X \btahat          \\
     & = \Y^T \Y - 2 \btahat^T \X^T \Y + \btahat^T \underbrace{\X^T \X \btahat}_{= \X^T \Y} \\
     & = \Y^T \Y - \btahat^T \X^T \Y                                                        \\
\end{align*}

% We would like a measure that indicates how far our predictions are from the measurements. We will use the prediction error for that purpose.

% \begin{definition}[Prediction error]
%     For a given set of observations \(\Y\), the \textbf{prediction error} is the squared \(\ell^2\)-norm of the difference between the prediction of our model and the observed value, that is
%     \[
%         \| \X \btahat - \X \bta \|_2^2
%     \]
% \end{definition}

\subsection{Maximum-likelihood estimation}
Let the error vector \(\eps\) be normally and independently distributed with constant variance \(\sgm^2\) (in other words, \(\eps\) is \(\mathscr{N}(0, \sgm^2 I)\)). Then the maximum-likelihood estimation model is given by
\[
    \Y = \X \bta + \eps
\]
where each component of the error vector has density function:
\[
    f \left(\eps_{i}\right) = \frac{1}{\sgm \sqrt{2 \pi}} \exp \left(-\frac{1}{2 \sgm^{2}} \eps_{i}^{2}\right), \quad i = 1, \ldots, n
\]
Subsequently, the likelihood function is the joint density of \(\eps_1, \ldots, \eps_n\), therefore it is given by
\begin{align*}
    L\left(\eps, \bta, \sgm^{2}\right)
     & = \prod_{i=1}^{n} f\left(\eps_{i}\right)                                                                        \\
     & = \left( \frac{1}{\sqrt{2 \pi} \sgm} \right)^n \exp \left(-\frac{1}{2 \sgm^{2}} \sum_{i=1}^{n} \eps_i^2 \right) \\
     & = \frac{1}{(2 \pi)^{n / 2} \sgm^{n}} \exp \left(-\frac{1}{2 \sgm^{2}} \eps^T \eps\right)                        \\
\end{align*}
which, using that \(\eps = \Y - \X \bta\), we may rewrite as
\[
    L\left(\Y, \X, \bta, \sgm^{2}\right) = \frac{1}{(2 \pi)^{n / 2} \sgm^{n}} \exp \left(-\frac{1}{2 \sgm^{2}} ( \Y - \X \bta )^T ( \Y - \X \bta )\right)
\]

We can apply the logarithm function on both sides of the above equation in order to cancel out the exponential and make our lives simpler for the rest of the analysis. Since the logarithm is an increasing function, it will not disturb the search for the argument maximizing \(L\). We call the result of the operation the log-likelihood.
\begin{align*}
      & \ln \left[ L\left(\Y, \X, \bta, \sgm^{2}\right) \right]                                                                           \\
    = & \ln \left[ \frac{1}{(2 \pi)^{n / 2} \sgm^{n}} \exp \left(-\frac{1}{2 \sgm^{2}} ( \Y - \X \bta )^T ( \Y - \X \bta )\right) \right] \\
    = & - \ln [ (2 \pi)^{n / 2} \sgm^{n} ] - \frac{1}{2 \sgm^{2}} ( \Y - \X \bta )^T ( \Y - \X \bta )                                     \\
    = & - \frac{n}{2} \ln(2 \pi) -n \ln(\sgm) - \frac{1}{2 \sgm^{2}} ( \Y - \X \bta )^T ( \Y - \X \bta )                                  \\
\end{align*}

Since \(\sgm\) is fixed, we can only let \(( \Y - \X \bta )^T ( \Y - \X \bta )\) vary. Naturally, we need to minimize it in order to maximize the log-likelihood. We already went through this procedure in \eqref{eqn: least-squares normal equations} and obtained in equation \eqref{eqn: least squares estimator} that the result is \(\btahat = (\X^T \X)^{-1} \X^T \Y\). Thus the maximum-likelihood estimator of \(\sgm^2\) is
\[
    \tilde \sgm^2 = \frac{( \Y - \X \btahat )^T ( \Y - \X \btahat )}{n}
\]