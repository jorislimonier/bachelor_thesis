\chapter{Classical theory of Linear Regression}

\section{Linear models}

We consider the setting of having a sample of $n$ observations
\[
    (\X_1, \Y_1), \ldots ,(\X_n, \Y_n)
\]
where $X_i \in \mathscr{X} \subseteq \R^p, \; i = 1, \ldots, n$ and $Y_i \in \mathscr{Y} \subseteq \R, \; i = 1, \ldots, n$.

\begin{definition}[The linear model]
    The relationship between an observation $\X_i \in \mathscr{X}$ and its outcome $\Y_i \in \mathscr{Y}$ can be established by a linear model, that is
    \begin{equation}
        i = 1, \ldots, n \qquad \Y_{i} = \sum_{j=1}^{p} \bta_{j} \X_{i}^{(j)} + \eps_{i}
    \end{equation}
    where \(\eps_1, \ldots, \eps_n\) are independent and identically distributed (i.i.d.). Moreover, \(\forall \, i = 1, \ldots, n, \) we have that \( \E [\eps_i] = 0\) and each \(\eps_i\) is independent of all of the \( X_j, \; j=1, \ldots, n \).
\end{definition}

Instead of seeing each observation individually we can deal with all of them together by expressing the linear model in matrix notation

\begin{equation}
    \label{eqn: linear model (matrix)}
    \Y = \X \bta + \eps
\end{equation}

\begin{definition}\
    \begin{enumerate}[label=(\alph*)]
        \item $\X$ is called the \textbf{design matrix}. It has dimension $n \times p$. $\X$ consists of stacking the vectors relative to each observation inside of a matrix
              \[
                  X =
                  \begin{bmatrix}
                      \text{---} & X_1^T  & \text{---} \\
                                 & \vdots &            \\
                      \text{---} & X_n^T  & \text{---} \\
                  \end{bmatrix}
              \]
        \item $\bta$ is called the \textbf{parameter vector}. It has dimension $p \times 1$.
        \item $\eps$ is called the \textbf{error vector}. It has dimension $n \times 1$.
        \item $\Y$ is called the \textbf{response vector}. It has dimension $n \times 1$.
    \end{enumerate}
\end{definition}

\section{The least squares method}

We define the objective function \( S(\bta) \) as follows
\begin{equation}
    \label{eqn: least squares objective function}
    S(\bta) := \eps^{T} \eps = (\Y-\X \bta)^{T}(\Y-\X \bta)
\end{equation}
which may be rewritten as
\begin{align*}
    S(\bta) & = (\Y - \X \bta)^{T} (\Y - \X \bta)                             \\
            & = \Y^T \Y - \Y^T \X \bta - \bta^T \X^T \Y + \bta^T \X^T \X \bta \\
            & = \Y^T \Y - 2 \bta^T \X^T \Y + \bta^T \X^T \X \bta              \\
\end{align*}
The least squares method aims at finding the vector $\btahat$ minimizing \( S \), that is
\[
    \btahat := \arg \min_{\bta} S(\bta)
\]
We find \( \btahat \) by differentiating \( S \) with respect to $\bta$ and setting the result to 0.
\begin{align}
             & \frac{\partial}{\partial \bta} S(\btahat) = 0 \nonumber                                                                  \\
    \implies & \frac{\partial}{\partial \btahat} \left( \Y^T \Y - 2 \btahat^T \X^T \Y + \btahat^T \X^T \X \btahat \right) = 0 \nonumber \\
    \implies & - 2 \X^T \Y + 2 \X^T \X \btahat = 0 \nonumber                                                                            \\
    \implies & \X^T \X \btahat = \X^T \Y \label{eqn: least-squares normal equations}
\end{align}
where equation \eqref{eqn: least-squares normal equations} is called the least squares normal equations.

If we assume that \( \X^T \X \) is invertible, then \eqref{eqn: least-squares normal equations} yields that our least squares estimator \( \btahat \) is given by
\begin{equation}
    \label{eqn: least squares estimator}
    \btahat = (\X^T \X)^{-1} \X^T \Y
\end{equation}
Now, we can verify that our estimator has some fundamental properties.
\begin{proposition}
    \(\btahat\) is unbiased, that is $\E [\btahat] = \bta$.
\end{proposition}
\begin{proof}
    \begin{align*}
        \E \left[ \btahat \right]
         & = \E \left[ \left(\X^{T} \X \right)^{-1} \X^{T} \Y \right]                                                \\
         & = \E \left[ \left(\X^{T} \X \right)^{-1} \X^{T}(\X \bta + \eps)\right]                                    \\
         & = \E \left[ \left(\X^{T} \X \right)^{-1} \X^{T} \X \bta + \left(\X^{T} \X\right)^{-1} \X^{T} \eps \right] \\
         & = \bta
    \end{align*}

\end{proof}
Moreover, we want to take a look at the estimator's variance. For this purpose we will use the covariance matrix. Let \(U, V \in \R^p\), recall that the covariance matrix is defined as
\[
    \cov (U, V) := \E \left[ \left( U - \E (U) \right) \left( V - \E (V) \right)^T \right] \in \mathcal{M}_{p \times p}(\R)
\]
where \(\forall \, i, j = 1, \ldots, p, \; \cov (U, V)_{ij}\) is the covariance between \( U_i \) and \( V_j\). In the particular case \( U = V \), the diagonal of the covariance matrix is nothing else than the variance of \( U \), that is:
\[
    \var (U)_i = \cov (U, U)_{ii} \quad i = 1, \ldots, p
\]
\begin{proposition}
    For \( \, i, j = 1, \ldots, p\), we have that:
    \begin{enumerate}[label=(\roman*)]
        \item \(\cov (\btahat_i, \btahat_j) = \sgm^{2} \left[ \left( \X^T \X \right)^{-1} \right]_{ij}\)
        \item \(\var (\btahat_i) = \sgm^{2} \left[ \left( \X^T \X \right)^{-1} \right]_{ii}\)
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}[label=(\roman*)]

        \item One can note that\begin{align*}
                  \cov(\btahat, \btahat)
                   & = \var \left[ \underbrace{\left(\X^T \X\right)^{-1} \X^T}_{\text{constant}} \Y\right]                               \\
                   & = \left(\X^T \X\right)^{-1} \X^T \underbrace{\var(\Y)}_{= \sgm^2 I} \left[\left( \X^T \X \right)^{-1} \X^T\right]^T \\
                   & = \sgm^{2}\left(\X^T \X\right)^{-1} \X^T \X\left(\X^T \X\right)^{-1}                                                \\
                   & = \sgm^{2}\left(\X^T \X\right)^{-1}
              \end{align*}
        \item This is a direct consequence of the first point.
    \end{enumerate}
\end{proof}

Now that we confirmed the above properties of \( \btahat \), we are interested in estimating the quality of our prediction. The residuals can help us do that.
\begin{definition}[Residuals]
    For a given set of observations \(\Y\), the \textbf{residuals} (or \textbf{vector of residuals}) is the difference between the prediction of our model and the observed value, that is
    \[
        X (\btahat - \bta) \in \R^n
    \]
\end{definition}

Building up from the residuals, we want to be able to estimate the value of \(\sgm^2\). We will develop such an estimator from the residual sum of squares.

\begin{definition}
    We defne the \textbf{residual sum of squares} \(\rss\) as follows
    \[
        \rss = \| \Y - \hat{\Y} \|_2^2 = \| \eps \|_2^2
    \]
\end{definition}

The residual sum of squares can be rewritten as follows:

\begin{align*}
    \rss
     & = \| \eps \|_2^2 \\
     & = \eps^T \eps \\
     & = \left( \Y - \X \btahat \right)^T \left(\Y - \X \btahat  \right) \\
     & = \Y^T \Y - \btahat^T \X^T \Y - \Y^T \X \btahat + \btahat^T \X^T \X \btahat \\
\end{align*}

We would like a measure that indicates how far our predictions are from the measurements. We will use the prediction error for that purpose.

\begin{definition}[Prediction error]
    For a given set of observations \(\Y\), the \textbf{prediction error} is the squared \(\ell^2\)-norm of the difference between the prediction of our model and the observed value, that is
    \[
        \| \X \btahat - \X \bta \|_2^2
    \]
\end{definition}



