\chapter{Introduction}

Regression is a sub-field of statistics, concerned with establishing a relationship between independent and dependent variables. The most common kind of regression is linear regression, where we establish such a relationship along a line. Regression analysis has two primary foci: first, prediction and forecasting, second, inference of causal relationships between independent and dependent variables. Linear regression has applications in many fields, which brings interest of a public with a variety of backgrounds. Most notably, applications can be found in machine learning, biology, bioinformatics, finance, economics, epidemiology, computer science, actuarial science, physics, chemistry, ...etc. \\
This thesis is separated into two chapters. In the first chapter we introduce the classical theory of linear regression and it is splitted into three sections. In the first section, we describe the problem that we are trying to solve, namely associating observations (data) with outputs. We define tools such as linear models, estimators and objective functions. Section two introduces the method of least squares, which aims at reducing the distance between predictions and our data. Then section three brings us the maximum likelihood estimation, where we work with a likelihood function, which aims at making the observed data most probable. In chapter two, we consider the LASSO, a technique to deal with a great number of parameters, compared to the number of observations. This is an uncomfortable case which may occur in the real world. We work with two section. In the first one, we consider the slightly heuristic case where the random part of the model has expected value of zero. Then in the second section, we lift that assumption and consider a (one step) more general case.
