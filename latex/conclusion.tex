\chapter{Conlusion}

The topic of this thesis is High Dimensional Regression Models, however we aimed at giving a good overview of Linear Regression in its more general form. A reader with some university level mathematics, but without prior knowledge of Linear Regression may feel challenged, but should not be lost by this thesis. We started from the very basic concepts of this field, defining the problem that linear regression solves and the situations in which it can be useful. We introduced building blocks such as linear models, estimators and objective functions. We presented two basic linear regression methods, which work for square systems, where the number of parameters equals the number of observations. The first one, the least squares method, minimizes its objective function which represents the distance between predictions and our data. After doing so, we presented the second method, the maximum likelihood estimation. It builds a framework under which the most probable outcome is the observed data. The rest was related to the core of this thesis: High Dimensional Regression Models. This time we considered cases where the number of parameters is (possibly very) large when compared to the number of observations. We used the LASSO to handle this change. The principle is simple but useful, we add a penalization term based on the \(\ell_1\) norm of the parameter vector. We firstly considered the somewhat heuristic case where the random part of our model is centered. We obtained several interesting results and tried to generalize them to the case where this assumption is not met. We saw that in general the formulas and theorems still conceptually hold but they require extra caution, namely to carry extra terms in order to account for the possibly non-zero expectation.